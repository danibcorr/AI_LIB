{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a implementar la arquitectura \"Inception\" empleada en GoogleLeNet\n",
    "# Al tratarse de una arquitectura más amplia vamos a modularizar en clases\n",
    "class CNN_Bloque(nn.Module):\n",
    "\n",
    "    def __init__(self, num_entradas, num_salidas, kernel_size, stride, padding):\n",
    "        \n",
    "        super(CNN_Bloque, self).__init__()\n",
    "        \n",
    "        self.activacion = nn.ReLU()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = num_entradas, out_channels = num_salidas, \n",
    "            kernel_size = kernel_size, stride = stride, padding = padding\n",
    "            )\n",
    "        \n",
    "        self.norm = nn.BatchNorm2d(num_features = num_salidas)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm(self.conv(x))\n",
    "        x = self.activacion(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Inception_Bloque(nn.Module):\n",
    "\n",
    "    # En el paper, en la tabla 1 se encuentra la estructura de la arquitectura\n",
    "    # En ellas se enceuntran las salidas, vamos a utilizar esas etiquetas para indicar las\n",
    "    # las salidas\n",
    "    def __init__(self, num_entradas, sal_1x1, sal_3x3_red, sal_3x3, sal_5x5_red, sal_5x5, sal_1x1_pool):\n",
    "        \n",
    "        super(Inception_Bloque, self).__init__()\n",
    "\n",
    "        # Modulo Inception con reducción en la dimension\n",
    "        # Son 4 ramas, interpretación de izquierda a derecha (rama 1, ..., rama 4)\n",
    "\n",
    "        self.rama1 = CNN_Bloque(\n",
    "            num_entradas = num_entradas, num_salidas= sal_1x1, \n",
    "            kernel_size = (1, 1)\n",
    "        )\n",
    "\n",
    "        self.rama2 = nn.Sequential(\n",
    "            CNN_Bloque(num_entradas = num_entradas, num_salidas = sal_3x3_red, kernel_size = (1, 1)),\n",
    "            CNN_Bloque(num_entradas = sal_3x3_red, num_salidas = sal_3x3, kernel_size = (3, 3), padding = 1)\n",
    "        )\n",
    "\n",
    "        self.rama3 = nn.Sequential(\n",
    "            CNN_Bloque(num_entradas = num_entradas, num_salidas = sal_5x5_red, kernel_size = (1, 1)),\n",
    "            CNN_Bloque(num_entradas = sal_5x5_red, num_salidas = sal_5x5, kernel_size = (5, 5), padding = 2)\n",
    "        )\n",
    "\n",
    "        self.rama4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size = (3, 3), stride = 1, padding = 1),\n",
    "            CNN_Bloque(num_entradas = num_entradas, num_salidas = sal_1x1_pool, kernel_size = (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Tenemos que concatenar el resultado de cada rama\n",
    "        rama1 = self.rama1(x)\n",
    "        rama2 = self.rama2(x)\n",
    "        rama3 = self.rama3(x)\n",
    "        rama4 = self.rama4(x)\n",
    "\n",
    "        filtro_concatenacion = torch.cat([rama1, rama2, rama3, rama4], dim = 1)\n",
    "\n",
    "        return filtro_concatenacion\n",
    "\n",
    "class GoogleLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_entradas):\n",
    "\n",
    "        super(GoogleLeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = CNN_Bloque(num_entradas = num_entradas, num_salidas = 64, kernel_size = (7, 7), stride = (2, 2), padding = (3, 3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = (3, 3), stride = (2, 2), padding = (1, 1))\n",
    "\n",
    "        self.conv2 = CNN_Bloque(num_entradas = 64, num_salidas = 192, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.pool2 = self.pool1\n",
    "\n",
    "        self.incep3a = Inception_Bloque(num_entradas = 192, sal_1x1 = 64, sal_3x3_red = 96, sal_3x3 = 128, sal_5x5_red = 16, sal_5x5 = 32, sal_1x1_pool = 32)\n",
    "        self.incep3b = Inception_Bloque(num_entradas = 256, sal_1x1 = 128, sal_3x3_red = 128, sal_3x3 = 192, sal_5x5_red = 32, sal_5x5 = 96, sal_1x1_pool = 64)\n",
    "        self.pool3 = self.pool1\n",
    "\n",
    "        self.incep4a = Inception_Bloque(num_entradas = 480, sal_1x1 = 192, sal_3x3_red = 96, sal_3x3 = 208, sal_5x5_red = 16, sal_5x5 = 48, sal_1x1_pool = 64)\n",
    "        self.incep4b = Inception_Bloque(num_entradas = 512, sal_1x1 = 160, sal_3x3_red = 112, sal_3x3 = 224, sal_5x5_red = 24, sal_5x5 = 64, sal_1x1_pool = 64)\n",
    "        self.incep4c = Inception_Bloque(num_entradas = 512, sal_1x1 = 128, sal_3x3_red = 128, sal_3x3 = 256, sal_5x5_red = 24, sal_5x5 = 64, sal_1x1_pool = 64)\n",
    "        self.incep4d = Inception_Bloque(num_entradas = 512, sal_1x1 = 112, sal_3x3_red = 144, sal_3x3 = 288, sal_5x5_red = 32, sal_5x5 = 64, sal_1x1_pool = 64)\n",
    "        self.incep4e = Inception_Bloque(num_entradas = 528, sal_1x1 = 256, sal_3x3_red = 160, sal_3x3 = 320, sal_5x5_red = 32, sal_5x5 = 128, sal_1x1_pool = 128)\n",
    "        self.pool4 = self.pool1\n",
    "\n",
    "        self.incep5a = Inception_Bloque(num_entradas = 832, sal_1x1 = 256, sal_3x3_red = 160, sal_3x3 = 320, sal_5x5_red = 32, sal_5x5 = 128, sal_1x1_pool = 128)\n",
    "        self.incep5b = Inception_Bloque(num_entradas = 832, sal_1x1 = 384, sal_3x3_red = 192, sal_3x3 = 384, sal_5x5_red = 48, sal_5x5 = 128, sal_1x1_pool = 128)\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size = (7, 7), stride = (1, 1))\n",
    "    \n",
    "        self.drop_out = nn.Dropout(p = 0.4)\n",
    "\n",
    "        self.fc = nn.Linear(in_features = 1024, out_features = 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        \n",
    "        x = self.pool2(self.conv2(x))\n",
    "\n",
    "        x = self.incep3a(x)\n",
    "        x = self.pool3(self.incep3b(x))\n",
    "\n",
    "        x = self.incep4a(x)\n",
    "        x = self.incep4b(x)\n",
    "        x = self.incep4c(x)\n",
    "        x = self.incep4d(x)\n",
    "        x = self.pool4(self.incep3e(x))\n",
    "\n",
    "        x = self.incep5a(x)\n",
    "        x = self.avg_pool(self.incep5b(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_1.7.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b93f5843423b5e3529b2348264279e864dd5c0e7e7f0b21d3f099c54aa317cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
